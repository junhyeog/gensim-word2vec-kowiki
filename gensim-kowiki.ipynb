{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install KoNLPy\n",
    "# https://mebadong.tistory.com/95\n",
    "# https://konlpy.org/ko/latest/install/\n",
    "\n",
    "## intall java\n",
    "# ! sudo apt update\n",
    "# ! sudo apt upgrade\n",
    "# ! sudo apt install openjdk-17-jdk\n",
    "## check java\n",
    "# ! java -version\n",
    "# ! javac -version\n",
    "## set environment\n",
    "# ! echo \"# >>> set JAVA environment\" >> ~/.zshrc\n",
    "################# ! echo \"export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))\" >> ~/.zshrc\n",
    "# ! echo \"export PATH=$PATH:$JAVA_HOME/bin\" >> ~/.zshrc\n",
    "# ! echo \"# <<< set JAVA environment\" >> ~/.zshrc\n",
    "# ! source ~/.zshrc\n",
    "## check environment\n",
    "# ! echo $JAVA_HOME\n",
    "## Install dependencies\n",
    "# ! sudo apt-get install g++ openjdk-8-jdk python3-dev python3-pip curl\n",
    "## Install KoNLPy\n",
    "# ! pip install --upgrade pip\n",
    "# ! pip install konlpy\n",
    "## Install MeCab\n",
    "# ! sudo apt-get -y install automake\n",
    "# ! zsh <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gensim\n",
    "# ! pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### downlad data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki dump data download\n",
    "# ! wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install wikiextractor and parse dump data\n",
    "# ! pip install wikiextractor\n",
    "# ! python -m wikiextractor.WikiExtractor kowiki-latest-pages-articles.xml.bz2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim-kowiki.ipynb                   kowiki.txt  \u001b[0m\u001b[01;34mw2v-models\u001b[0m/\n",
      "\u001b[01;31mkowiki-latest-pages-articles.xml.bz2\u001b[0m  \u001b[01;34mtext\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AA', 'AB', 'AC', 'AD', 'AE', 'AF', 'AG', 'AH', 'AI', 'AJ']\n",
      "['wiki_00', 'wiki_01', 'wiki_02', 'wiki_03', 'wiki_04', 'wiki_05', 'wiki_06', 'wiki_07', 'wiki_08', 'wiki_09', 'wiki_10', 'wiki_11', 'wiki_12', 'wiki_13', 'wiki_14', 'wiki_15', 'wiki_16', 'wiki_17', 'wiki_18', 'wiki_19', 'wiki_20', 'wiki_21', 'wiki_22', 'wiki_23', 'wiki_24', 'wiki_25', 'wiki_26', 'wiki_27', 'wiki_28', 'wiki_29', 'wiki_30', 'wiki_31', 'wiki_32', 'wiki_33', 'wiki_34', 'wiki_35', 'wiki_36', 'wiki_37', 'wiki_38', 'wiki_39', 'wiki_40', 'wiki_41', 'wiki_42', 'wiki_43', 'wiki_44', 'wiki_45', 'wiki_46', 'wiki_47', 'wiki_48', 'wiki_49', 'wiki_50', 'wiki_51', 'wiki_52', 'wiki_53', 'wiki_54', 'wiki_55', 'wiki_56', 'wiki_57', 'wiki_58', 'wiki_59', 'wiki_60', 'wiki_61', 'wiki_62', 'wiki_63', 'wiki_64', 'wiki_65', 'wiki_66', 'wiki_67', 'wiki_68', 'wiki_69', 'wiki_70', 'wiki_71', 'wiki_72', 'wiki_73', 'wiki_74', 'wiki_75', 'wiki_76', 'wiki_77', 'wiki_78', 'wiki_79', 'wiki_80', 'wiki_81', 'wiki_82', 'wiki_83', 'wiki_84', 'wiki_85', 'wiki_86', 'wiki_87', 'wiki_88', 'wiki_89', 'wiki_90', 'wiki_91', 'wiki_92', 'wiki_93', 'wiki_94', 'wiki_95', 'wiki_96', 'wiki_97', 'wiki_98', 'wiki_99']\n"
     ]
    }
   ],
   "source": [
    "# check text dir\n",
    "print(os.listdir(\"text\"))\n",
    "print(os.listdir(\"text/AA\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<doc id=\"77601\" url=\"https://ko.wikipedia.org/wiki?curid=77601\" title=\"소학언해\">\n",
      "\n",
      "소학언해\n",
      "\n",
      "\n",
      "\n",
      "소학언해(小學諺解)는 중국의 주자가 지은 《소학》(小學)을 조선 선조 20년인 1587년에 정구 등 학자들이 한국어로 언해한 책이다. 15세기에 제작된 훈민정음 이후 변화된, 16세기 말 한국어의 모습을 잘 보여준다.\n",
      "\n",
      "서.\n",
      "\n",
      "...\n",
      "대문자 O 표기법\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "</doc>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"text/AA/wiki_99\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[:5]:\n",
    "        print(line)\n",
    "    print(\"...\")\n",
    "    for line in lines[-5:]:\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912\n"
     ]
    }
   ],
   "source": [
    "# get all wifi file's path\n",
    "def list_wiki(dirname):\n",
    "    filepaths = []\n",
    "    filenames = os.listdir(dirname)\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(dirname, filename)\n",
    "\n",
    "        if os.path.isdir(filepath):\n",
    "            filepaths.extend(list_wiki(filepath))\n",
    "        else:\n",
    "            find = re.findall(r\"wiki_[0-9][0-9]\", filepath)\n",
    "            if 0 < len(find):\n",
    "                filepaths.append(filepath)\n",
    "    return sorted(filepaths)\n",
    "\n",
    "\n",
    "filepaths = list_wiki(\"text\")\n",
    "print(len(filepaths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 912/912 [00:12<00:00, 74.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.06 s, sys: 7.67 s, total: 16.7 s\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# concat all wiki files to kowiki.txt\n",
    "concated_file = \"kowiki.txt\"\n",
    "with open(concated_file, \"w\") as outfile:\n",
    "    for filename in tqdm(filepaths):\n",
    "        with open(filename) as infile:\n",
    "            contents = infile.read()\n",
    "            outfile.write(contents)\n",
    "\n",
    "f = open(concated_file, \"r\")\n",
    "lines = f.read().splitlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10364934\n",
      "['<doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">', '지미 카터', '', '제임스 얼 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39대 대통령 (1977년 ~ 1981년)이다.', '생애.']\n"
     ]
    }
   ],
   "source": [
    "print(len(lines))\n",
    "print(lines[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10364934/10364934 [00:02<00:00, 4005630.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7001189\n",
      "<doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\n",
      "지미 카터\n",
      "제임스 얼 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39대 대통령 (1977년 ~ 1981년)이다.\n",
      "생애.\n",
      "어린 시절.\n",
      "CPU times: user 2.17 s, sys: 422 ms, total: 2.59 s\n",
      "Wall time: 2.59 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# erase empty lines\n",
    "non_empty_lines = []\n",
    "for line in tqdm(lines):\n",
    "    if line:\n",
    "        non_empty_lines.append(line)\n",
    "print(len(non_empty_lines))\n",
    "for line in non_empty_lines[:5]:\n",
    "    print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization with mecab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7001189/7001189 [15:25<00:00, 7561.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7001189\n",
      "CPU times: user 11min 31s, sys: 2min 47s, total: 14min 19s\n",
      "Wall time: 15min 25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenization\n",
    "mecab = Mecab()\n",
    "\n",
    "result = []\n",
    "for line in tqdm(non_empty_lines):\n",
    "    result.append(mecab.morphs(line))\n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<', 'doc', 'id', '=\"', '5', '\"', 'url', '=\"', 'https', ':', '/', '/', 'ko', '.', 'wikipedia', '.', 'org', '/', 'wiki', '?', 'curid', '=', '5', '\"', 'title', '=\"', '지미', '카터', '\">'], ['지미', '카터'], ['제임스', '얼', '카터', '주니어', '(', ',', '1924', '년', '10', '월', '1', '일', '~', ')', '는', '민주당', '출신', '미국', '39', '대', '대통령', '(', '1977', '년', '~', '1981', '년', ')', '이', '다', '.'], ['생애', '.'], ['어린', '시절', '.'], ['지미', '카터', '는', '조지', '아주', '섬터', '카운티', '플', '레인스', '마을', '에서', '태어났', '다', '.'], ['조지아', '공과', '대학교', '를', '졸업', '하', '였', '다', '.', '그', '후', '해군', '에', '들어가', '전함', '·', '원자력', '·', '잠수함', '의', '승무원', '으로', '일', '하', '였', '다', '.', '1953', '년', '미국', '해군', '대위', '로', '예편', '하', '였', '고', '이후', '땅콩', '·', '면화', '등', '을', '가꿔', '많', '은', '돈', '을', '벌', '었', '다', '.', '그', '의', '별명', '이', '\"', '땅콩', '농부', '\"', '(', 'Peanut', 'Farmer', ')', '로', '알려졌', '다', '.'], ['정계', '입문', '.'], ['1962', '년', '조지', '아주', '상원', '의원', '선거', '에서', '낙선', '하', '나', '그', '선거', '가', '부정', '선거', '였음을', '입증', '하', '게', '되', '어', '당선', '되', '고', ',', '1966', '년', '조지아', '주지사', '선거', '에', '낙선', '하', '지만', ',', '1970', '년', '조지아', '주지사', '를', '역임', '했', '다', '.', '대통령', '이', '되', '기', '전', '조지', '아주', '상원', '의원', '을', '두', '번', '연임', '했으며', ',', '1971', '년', '부터', '1975', '년', '까지', '조지', '아', '지사', '로', '근무', '했', '다', '.', '조지', '아', '주지사', '로', '지내', '면서', ',', '미국', '에', '사', '는', '흑인', '등', '용법', '을', '내세웠', '다', '.'], ['대통령', '재임', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(result[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://radimrehurek.com/gensim/models/word2vec.html?highlight=word2vec#module-gensim.models.word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49min 22s, sys: 5min 7s, total: 54min 30s\n",
      "Wall time: 19min 38s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_cbow_5_10 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=5,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=0,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=10,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_cbow_5_10.save(\"w2v-models/model_cbow_5_10.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54min 35s, sys: 5min 8s, total: 59min 44s\n",
      "Wall time: 19min 54s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_cbow_15_10 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=15,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=0,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=10,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_cbow_15_10.save(\"w2v-models/model_cbow_15_10.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 4min 19s, sys: 5min 30s, total: 1h 9min 49s\n",
      "Wall time: 21min 29s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_cbow_30_10 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=30,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=0,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=10,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_cbow_30_10.save(\"w2v-models/model_cbow_30_10.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 46min 49s, sys: 6min 23s, total: 3h 53min 12s\n",
      "Wall time: 33min 11s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_sg_5_10 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=5,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=1,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=10,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_sg_5_10.save(\"w2v-models/model_sg_5_10.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8h 58min 21s, sys: 6min 33s, total: 9h 4min 54s\n",
      "Wall time: 1h 4min 50s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_sg_15_10 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=15,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=1,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=10,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_sg_15_10.save(\"w2v-models/model_sg_15_10.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15h 8min 50s, sys: 5min 50s, total: 15h 14min 41s\n",
      "Wall time: 1h 41min 35s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_sg_30_10 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=30,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=1,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=10,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_sg_30_10.save(\"w2v-models/model_sg_30_10.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 14s, sys: 3min 1s, total: 8min 16s\n",
      "Wall time: 10min 31s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_sg_15_0 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=15,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=1,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=0,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_sg_15_0.save(\"w2v-models/model_sg_15_0.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 10min 20s, sys: 4min 17s, total: 2h 14min 37s\n",
      "Wall time: 19min 25s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_sg_15_2 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=15,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=1,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=2,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_sg_15_2.save(\"w2v-models/model_sg_15_2.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 34min, sys: 5min 22s, total: 4h 39min 23s\n",
      "Wall time: 32min 24s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_sg_15_5 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=15,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=1,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=5,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_sg_15_5.save(\"w2v-models/model_sg_15_5.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16h 16min 43s, sys: 4min 35s, total: 16h 21min 19s\n",
      "Wall time: 1h 42min 2s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# model_sg_15_20 = Word2Vec(\n",
    "#     sentences=result,\n",
    "#     vector_size=100,  # 100, Dimensionality of the word vectors.\n",
    "#     window=15,  # Maximum distance between the current and predicted word within a sentence.\n",
    "#     min_count=5,  # Ignores all words with total frequency lower than this.\n",
    "#     workers=10,\n",
    "#     sg=1,  # Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "#     negative=20,  # If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used.\n",
    "#     epochs=5,  # 5, Number of iterations (epochs) over the corpus.\n",
    "#     batch_words=10000,  # 10000, Target size (in words) for batches of examples passed to worker threads\n",
    "#     compute_loss=True,  #  If True, computes and stores loss value which can be retrieved using get_latest_training_loss().\n",
    "# )\n",
    "\n",
    "# model_sg_15_20.save(\"w2v-models/model_sg_15_20.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[]\n",
    "models.append(Word2Vec.load(\"w2v-models/model_cbow_5_10.model\"))\n",
    "models.append(Word2Vec.load(\"w2v-models/model_cbow_15_10.model\"))\n",
    "models.append(Word2Vec.load(\"w2v-models/model_cbow_30_10.model\"))\n",
    "models.append(Word2Vec.load(\"w2v-models/model_sg_5_10.model\"))\n",
    "models.append(Word2Vec.load(\"w2v-models/model_sg_15_10.model\"))\n",
    "models.append(Word2Vec.load(\"w2v-models/model_sg_30_10.model\"))\n",
    "models.append(Word2Vec.load(\"w2v-models/model_sg_15_0.model\"))\n",
    "models.append(Word2Vec.load(\"w2v-models/model_sg_15_2.model\"))\n",
    "models.append(Word2Vec.load(\"w2v-models/model_sg_15_5.model\"))\n",
    "models.append(Word2Vec.load(\"w2v-models/model_sg_15_20.model\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] sg: 0, window: 5, negative: 10\n",
      "굉장        : 0.91\n",
      "특출        : 0.811\n",
      "어마어마      : 0.79\n",
      "훌륭        : 0.782\n",
      "암울        : 0.772\n",
      "\n",
      "[+] sg: 0, window: 15, negative: 10\n",
      "굉장        : 0.9\n",
      "훌륭        : 0.832\n",
      "어마어마      : 0.795\n",
      "탁월        : 0.759\n",
      "특출        : 0.744\n",
      "\n",
      "[+] sg: 0, window: 30, negative: 10\n",
      "굉장        : 0.878\n",
      "훌륭        : 0.84\n",
      "어마어마      : 0.781\n",
      "탁월        : 0.743\n",
      "막강        : 0.731\n",
      "\n",
      "[+] sg: 1, window: 5, negative: 10\n",
      "굉장        : 0.866\n",
      "훌륭        : 0.779\n",
      "특출        : 0.77\n",
      "놀라운       : 0.729\n",
      "놀랄        : 0.724\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 10\n",
      "굉장        : 0.867\n",
      "훌륭        : 0.773\n",
      "놀랄        : 0.758\n",
      "놀라운       : 0.743\n",
      "감탄        : 0.737\n",
      "\n",
      "[+] sg: 1, window: 30, negative: 10\n",
      "굉장        : 0.869\n",
      "훌륭        : 0.795\n",
      "너무나       : 0.783\n",
      "평했        : 0.768\n",
      "극찬        : 0.763\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 0\n",
      "러프버러      : 0.452\n",
      "仁恕        : 0.429\n",
      "lanceolatus: 0.421\n",
      "콧         : 0.409\n",
      "Sno       : 0.406\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 2\n",
      "굉장        : 0.843\n",
      "훌륭        : 0.778\n",
      "인상깊       : 0.738\n",
      "놀라운       : 0.737\n",
      "감탄        : 0.735\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 5\n",
      "굉장        : 0.855\n",
      "훌륭        : 0.775\n",
      "감탄        : 0.757\n",
      "정말        : 0.734\n",
      "놀랄        : 0.731\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 20\n",
      "굉장        : 0.887\n",
      "놀랄        : 0.789\n",
      "훌륭        : 0.778\n",
      "감탄        : 0.761\n",
      "놀라운       : 0.752\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(f\"\\n[+] sg: {model.sg}, window: {model.window}, negative: {model.negative}\")\n",
    "    for w in model.wv.most_similar(positive=[\"대단\"], negative=[], topn=5):\n",
    "        print(f\"{w[0]:10}: {round(w[1], 3)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] sg: 0, window: 5, negative: 10\n",
      "괜찮        : 0.777\n",
      "높         : 0.761\n",
      "많         : 0.756\n",
      "낮         : 0.739\n",
      "귀찮        : 0.709\n",
      "\n",
      "[+] sg: 0, window: 15, negative: 10\n",
      "괜찮        : 0.808\n",
      "많         : 0.759\n",
      "높         : 0.752\n",
      "낮         : 0.718\n",
      "귀찮        : 0.699\n",
      "\n",
      "[+] sg: 0, window: 30, negative: 10\n",
      "괜찮        : 0.817\n",
      "많         : 0.748\n",
      "높         : 0.739\n",
      "낮         : 0.705\n",
      "귀찮        : 0.62\n",
      "\n",
      "[+] sg: 1, window: 5, negative: 10\n",
      "괜찮        : 0.824\n",
      "나쁘        : 0.767\n",
      "녹록        : 0.746\n",
      "윤건희       : 0.728\n",
      "그닥        : 0.713\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 10\n",
      "괜찮        : 0.826\n",
      "잘         : 0.775\n",
      "윤건희       : 0.719\n",
      "들을수록      : 0.708\n",
      "해졌으면      : 0.703\n",
      "\n",
      "[+] sg: 1, window: 30, negative: 10\n",
      "잘         : 0.798\n",
      "보여        : 0.737\n",
      "괜찮        : 0.737\n",
      "생각        : 0.734\n",
      "보         : 0.719\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 0\n",
      "Kafka     : 0.461\n",
      "언소        : 0.456\n",
      "원생생물      : 0.45\n",
      "千年        : 0.438\n",
      "베른슈타인     : 0.423\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 2\n",
      "잘         : 0.794\n",
      "괜찮        : 0.787\n",
      "보여        : 0.702\n",
      "싶         : 0.693\n",
      "못         : 0.691\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 5\n",
      "잘         : 0.785\n",
      "괜찮        : 0.781\n",
      "보         : 0.702\n",
      "너무        : 0.696\n",
      "윤건희       : 0.694\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 20\n",
      "괜찮        : 0.808\n",
      "잘         : 0.747\n",
      "윤건희       : 0.733\n",
      "들을수록      : 0.729\n",
      "해졌으면      : 0.718\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(f\"\\n[+] sg: {model.sg}, window: {model.window}, negative: {model.negative}\")\n",
    "    for w in model.wv.most_similar(positive=[\"좋\"], negative=[], topn=5):\n",
    "        print(f\"{w[0]:10}: {round(w[1], 3)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] sg: 0, window: 5, negative: 10\n",
      "걱정        : 0.641\n",
      "못하        : 0.632\n",
      "피곤        : 0.616\n",
      "괴롭        : 0.61\n",
      "가려        : 0.593\n",
      "\n",
      "[+] sg: 0, window: 15, negative: 10\n",
      "못하        : 0.655\n",
      "걱정        : 0.628\n",
      "무서워       : 0.618\n",
      "가려        : 0.615\n",
      "막막        : 0.608\n",
      "\n",
      "[+] sg: 0, window: 30, negative: 10\n",
      "무서워       : 0.649\n",
      "괴롭        : 0.635\n",
      "걱정        : 0.629\n",
      "못하        : 0.626\n",
      "멀쩡        : 0.621\n",
      "\n",
      "[+] sg: 1, window: 5, negative: 10\n",
      "미더워       : 0.781\n",
      "마땅        : 0.778\n",
      "다칠까       : 0.775\n",
      "뻔한데       : 0.762\n",
      "괜한        : 0.757\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 10\n",
      "그래도       : 0.779\n",
      "괜찮        : 0.764\n",
      "뻔한데       : 0.759\n",
      "그렇게       : 0.74\n",
      "질까봐       : 0.74\n",
      "\n",
      "[+] sg: 1, window: 30, negative: 10\n",
      "그래도       : 0.783\n",
      "너무        : 0.773\n",
      "그런데       : 0.773\n",
      "조금        : 0.772\n",
      "그렇게       : 0.768\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 0\n",
      "천둥        : 0.452\n",
      "빛살        : 0.446\n",
      "편향성       : 0.423\n",
      "김공        : 0.418\n",
      "Fast      : 0.415\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 2\n",
      "그래도       : 0.788\n",
      "괜찮        : 0.772\n",
      "그런다       : 0.74\n",
      "너무        : 0.737\n",
      "콧방귀       : 0.73\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 5\n",
      "그래도       : 0.772\n",
      "괜찮        : 0.756\n",
      "못하        : 0.745\n",
      "그렇게       : 0.743\n",
      "너무        : 0.74\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 20\n",
      "그래도       : 0.809\n",
      "괜찮        : 0.777\n",
      "괜한        : 0.756\n",
      "권해서       : 0.753\n",
      "라더니       : 0.749\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(f\"\\n[+] sg: {model.sg}, window: {model.window}, negative: {model.negative}\")\n",
    "    for w in model.wv.most_similar(positive=[\"못\"], negative=[], topn=5):\n",
    "        print(f\"{w[0]:10}: {round(w[1], 3)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] sg: 0, window: 5, negative: 10\n",
      "많이        : 0.775\n",
      "널리        : 0.637\n",
      "오래        : 0.617\n",
      "자주        : 0.614\n",
      "제대로       : 0.606\n",
      "\n",
      "[+] sg: 0, window: 15, negative: 10\n",
      "많이        : 0.743\n",
      "오래        : 0.589\n",
      "널리        : 0.586\n",
      "자주        : 0.571\n",
      "섬세        : 0.56\n",
      "\n",
      "[+] sg: 0, window: 30, negative: 10\n",
      "많이        : 0.703\n",
      "뛰어나       : 0.586\n",
      "자주        : 0.554\n",
      "솜씨        : 0.55\n",
      "똑똑        : 0.548\n",
      "\n",
      "[+] sg: 1, window: 5, negative: 10\n",
      "많이        : 0.714\n",
      "제대로       : 0.692\n",
      "휼륭한       : 0.687\n",
      "좋         : 0.679\n",
      "오래        : 0.673\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 10\n",
      "좋         : 0.775\n",
      "매우        : 0.771\n",
      "많이        : 0.748\n",
      "처럼        : 0.745\n",
      "알려져       : 0.743\n",
      "\n",
      "[+] sg: 1, window: 30, negative: 10\n",
      "매우        : 0.864\n",
      "알려져       : 0.841\n",
      "처럼        : 0.805\n",
      "좋         : 0.798\n",
      "보다        : 0.797\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 0\n",
      "右肩        : 0.482\n",
      "가까운가      : 0.446\n",
      "급부상       : 0.41\n",
      "그대와       : 0.409\n",
      "after     : 0.401\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 2\n",
      "좋         : 0.794\n",
      "매우        : 0.784\n",
      "많이        : 0.761\n",
      "처럼        : 0.744\n",
      "어서        : 0.742\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 5\n",
      "매우        : 0.795\n",
      "좋         : 0.785\n",
      "많이        : 0.775\n",
      "알려져       : 0.756\n",
      "처럼        : 0.736\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 20\n",
      "매우        : 0.773\n",
      "많이        : 0.767\n",
      "좋         : 0.747\n",
      "처럼        : 0.742\n",
      "만큼        : 0.73\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(f\"\\n[+] sg: {model.sg}, window: {model.window}, negative: {model.negative}\")\n",
    "    for w in model.wv.most_similar(positive=[\"잘\"], negative=[], topn=5):\n",
    "        print(f\"{w[0]:10}: {round(w[1], 3)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] sg: 0, window: 5, negative: 10\n",
      "잘         : 0.775\n",
      "자주        : 0.773\n",
      "널리        : 0.715\n",
      "큰         : 0.703\n",
      "많         : 0.642\n",
      "\n",
      "[+] sg: 0, window: 15, negative: 10\n",
      "잘         : 0.743\n",
      "자주        : 0.739\n",
      "큰         : 0.683\n",
      "널리        : 0.662\n",
      "많         : 0.644\n",
      "\n",
      "[+] sg: 0, window: 30, negative: 10\n",
      "잘         : 0.703\n",
      "자주        : 0.69\n",
      "큰         : 0.645\n",
      "많         : 0.643\n",
      "흔한        : 0.623\n",
      "\n",
      "[+] sg: 1, window: 5, negative: 10\n",
      "자주        : 0.784\n",
      "많         : 0.771\n",
      "잘         : 0.714\n",
      "비교적       : 0.702\n",
      "읽혀진       : 0.699\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 10\n",
      "많         : 0.783\n",
      "매우        : 0.769\n",
      "대부분       : 0.767\n",
      "특히        : 0.749\n",
      "잘         : 0.748\n",
      "\n",
      "[+] sg: 1, window: 30, negative: 10\n",
      "대부분       : 0.816\n",
      "매우        : 0.811\n",
      "비교        : 0.801\n",
      "높         : 0.79\n",
      "잘         : 0.783\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 0\n",
      "SLOW      : 0.451\n",
      "레오폴드      : 0.416\n",
      "덴젤        : 0.413\n",
      "Forgive   : 0.411\n",
      "Sailfish  : 0.411\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 2\n",
      "매우        : 0.796\n",
      "대부분       : 0.784\n",
      "많         : 0.768\n",
      "잘         : 0.761\n",
      "높         : 0.757\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 5\n",
      "매우        : 0.799\n",
      "대부분       : 0.785\n",
      "잘         : 0.775\n",
      "많         : 0.764\n",
      "특히        : 0.762\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 20\n",
      "많         : 0.799\n",
      "대부분       : 0.784\n",
      "특히        : 0.769\n",
      "잘         : 0.767\n",
      "매우        : 0.76\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(f\"\\n[+] sg: {model.sg}, window: {model.window}, negative: {model.negative}\")\n",
    "    for w in model.wv.most_similar(positive=[\"많이\"], negative=[], topn=5):\n",
    "        print(f\"{w[0]:10}: {round(w[1], 3)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] sg: 0, window: 5, negative: 10\n",
      "가끔        : 0.793\n",
      "약간        : 0.786\n",
      "이따금       : 0.747\n",
      "반반        : 0.62\n",
      "살짝        : 0.616\n",
      "\n",
      "[+] sg: 0, window: 15, negative: 10\n",
      "이따금       : 0.749\n",
      "가끔        : 0.749\n",
      "약간        : 0.698\n",
      "반반        : 0.573\n",
      "한참        : 0.546\n",
      "\n",
      "[+] sg: 0, window: 30, negative: 10\n",
      "약간        : 0.735\n",
      "이따금       : 0.703\n",
      "가끔        : 0.696\n",
      "살짝        : 0.575\n",
      "다소        : 0.519\n",
      "\n",
      "[+] sg: 1, window: 5, negative: 10\n",
      "약간        : 0.84\n",
      "꽤         : 0.758\n",
      "상당히       : 0.741\n",
      "나아진다      : 0.739\n",
      "달라져       : 0.738\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 10\n",
      "약간        : 0.8\n",
      "꽤         : 0.761\n",
      "굉장히       : 0.753\n",
      "상당히       : 0.748\n",
      "다소        : 0.728\n",
      "\n",
      "[+] sg: 1, window: 30, negative: 10\n",
      "약간        : 0.811\n",
      "너무        : 0.786\n",
      "꽤         : 0.78\n",
      "상당히       : 0.774\n",
      "못         : 0.772\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 0\n",
      "로프그렌      : 0.427\n",
      "웅탄        : 0.419\n",
      "차임        : 0.415\n",
      "정신상       : 0.414\n",
      "칼도        : 0.408\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 2\n",
      "약간        : 0.797\n",
      "너무        : 0.762\n",
      "꽤         : 0.754\n",
      "만큼        : 0.743\n",
      "상당히       : 0.737\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 5\n",
      "약간        : 0.806\n",
      "너무        : 0.737\n",
      "꽤         : 0.737\n",
      "다소        : 0.735\n",
      "상당히       : 0.727\n",
      "\n",
      "[+] sg: 1, window: 15, negative: 20\n",
      "약간        : 0.802\n",
      "꽤         : 0.744\n",
      "상당히       : 0.733\n",
      "다소        : 0.729\n",
      "굉장히       : 0.729\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(f\"\\n[+] sg: {model.sg}, window: {model.window}, negative: {model.negative}\")\n",
    "    for w in model.wv.most_similar(positive=[\"조금\"], negative=[], topn=5):\n",
    "        print(f\"{w[0]:10}: {round(w[1], 3)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[0] sg: 0, window: 5, negative: 10\n",
      "놀란다       : 0.557\n",
      "무심코       : 0.551\n",
      "뭔가        : 0.55\n",
      "어쩐지       : 0.546\n",
      "매번        : 0.545\n",
      "\n",
      "[1] sg: 0, window: 15, negative: 10\n",
      "놀란다       : 0.642\n",
      "장난        : 0.626\n",
      "뭔가        : 0.621\n",
      "해한다       : 0.611\n",
      "정말        : 0.605\n",
      "\n",
      "[2] sg: 0, window: 30, negative: 10\n",
      "해한다       : 0.623\n",
      "정말        : 0.61\n",
      "멋진        : 0.608\n",
      "놀랐        : 0.608\n",
      "뭔가        : 0.608\n",
      "\n",
      "[3] sg: 1, window: 5, negative: 10\n",
      "놀랬        : 0.796\n",
      "터뜨린다      : 0.732\n",
      "어리둥절      : 0.731\n",
      "열받        : 0.731\n",
      "어김없이      : 0.718\n",
      "\n",
      "[4] sg: 1, window: 15, negative: 10\n",
      "놀랬        : 0.794\n",
      "긴가민가      : 0.779\n",
      "울컥        : 0.773\n",
      "놀랐        : 0.771\n",
      "오랜만       : 0.76\n",
      "\n",
      "[5] sg: 1, window: 30, negative: 10\n",
      "오랜만       : 0.819\n",
      "울컥        : 0.794\n",
      "그리웠       : 0.787\n",
      "정말        : 0.783\n",
      "은데요       : 0.777\n",
      "\n",
      "[6] sg: 1, window: 15, negative: 0\n",
      "문무        : 0.421\n",
      "소완규       : 0.411\n",
      "배닝        : 0.405\n",
      "포강        : 0.403\n",
      "김석휘       : 0.399\n",
      "\n",
      "[7] sg: 1, window: 15, negative: 2\n",
      "글썽이       : 0.766\n",
      "울먹이       : 0.762\n",
      "시큰둥       : 0.754\n",
      "오래간만      : 0.751\n",
      "오랜만       : 0.745\n",
      "\n",
      "[8] sg: 1, window: 15, negative: 5\n",
      "긴가민가      : 0.777\n",
      "놀랐        : 0.767\n",
      "기뻤        : 0.763\n",
      "어땠을까      : 0.762\n",
      "놀라        : 0.758\n",
      "\n",
      "[9] sg: 1, window: 15, negative: 20\n",
      "놀라        : 0.787\n",
      "놀랬        : 0.778\n",
      "놀랐        : 0.775\n",
      "오랜만       : 0.77\n",
      "어리둥절      : 0.768\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f\"\\n[{i}] sg: {model.sg}, window: {model.window}, negative: {model.negative}\")\n",
    "    for w in model.wv.most_similar(positive=[\"깜짝\"], negative=[], topn=5):\n",
    "        print(f\"{w[0]:10}: {round(w[1], 3)}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('airush22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac7ee0b993c5a35fa1857608df0d7962e293b7d580915ad7aa4577b341aefb4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
